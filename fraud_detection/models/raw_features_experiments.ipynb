{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "634757bb",
   "metadata": {},
   "source": [
    "### Model BASELINE training with raw dataset features (no risk scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ed6863",
   "metadata": {},
   "source": [
    "##### Import processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "397d7efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            customer_id          transaction_timestamp  \\\n",
      "0  ab47a4be-d92d-499f-b6f3-9c07dcd948e5  2020-12-09 07:48:01.897480452   \n",
      "1  ab47a4be-d92d-499f-b6f3-9c07dcd948e5  2021-01-09 17:07:54.659347898   \n",
      "2  ab47a4be-d92d-499f-b6f3-9c07dcd948e5  2022-02-24 02:34:58.503224288   \n",
      "3  ab47a4be-d92d-499f-b6f3-9c07dcd948e5  2022-05-01 02:52:46.265885528   \n",
      "4  ab47a4be-d92d-499f-b6f3-9c07dcd948e5  2020-11-20 06:59:33.669327084   \n",
      "\n",
      "   transaction_amount merchant_category transaction_status  \\\n",
      "0               12.92         groceries          completed   \n",
      "1               17.18            travel          completed   \n",
      "2               27.02       restaurants          completed   \n",
      "3                4.59          clothing          completed   \n",
      "4               14.56          clothing          completed   \n",
      "\n",
      "                         transaction_id   channel   entry_mode  \\\n",
      "0  9274be52-c787-4d7c-a09b-1559b5d20fd1  in-store  contactless   \n",
      "1  15483ab8-6985-4175-9ad0-91b8c32b0a6f    mobile       manual   \n",
      "2  d79c0405-3f51-4ce4-ba58-663fde16ea7d    mobile  contactless   \n",
      "3  f7850839-9bb5-4fbb-8dbb-7b4fe9e5fadd    online       manual   \n",
      "4  e0c4d755-905c-4007-916f-b8c65b54acc7    online  contactless   \n",
      "\n",
      "  transaction_country  is_international  ... is_weekend  is_night  \\\n",
      "0                  US             False  ...      False     False   \n",
      "1                  US             False  ...       True     False   \n",
      "2                  US             False  ...      False      True   \n",
      "3                  US             False  ...       True      True   \n",
      "4                  US             False  ...      False     False   \n",
      "\n",
      "   monthly_income amount_vs_income_ratio category_risk_score  \\\n",
      "0          1250.0               0.010336                 0.5   \n",
      "1          1250.0               0.013744                 2.0   \n",
      "2          1250.0               0.021616                 0.7   \n",
      "3          1250.0               0.003672                 0.8   \n",
      "4          1250.0               0.011648                 0.8   \n",
      "\n",
      "   channel_risk_score  entry_mode_risk_score  country_risk_score  fraud_score  \\\n",
      "0                 0.7                    1.0                 1.0          3.0   \n",
      "1                 1.5                    3.0                 1.0         11.8   \n",
      "2                 1.5                    1.0                 1.0          5.8   \n",
      "3                 2.0                    3.0                 1.0         10.4   \n",
      "4                 2.0                    1.0                 1.0          5.4   \n",
      "\n",
      "   fraud_label  \n",
      "0            0  \n",
      "1            1  \n",
      "2            0  \n",
      "3            0  \n",
      "4            0  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read CSV and transform to dataframe\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'..\\..\\data\\processed\\dataset.csv')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a619aa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3399996 entries, 0 to 3399995\n",
      "Data columns (total 27 columns):\n",
      " #   Column                  Dtype  \n",
      "---  ------                  -----  \n",
      " 0   customer_id             object \n",
      " 1   transaction_timestamp   object \n",
      " 2   transaction_amount      float64\n",
      " 3   merchant_category       object \n",
      " 4   transaction_status      object \n",
      " 5   transaction_id          object \n",
      " 6   channel                 object \n",
      " 7   entry_mode              object \n",
      " 8   transaction_country     object \n",
      " 9   is_international        bool   \n",
      " 10  customer_name           object \n",
      " 11  age                     int64  \n",
      " 12  income                  float64\n",
      " 13  signup_date             object \n",
      " 14  region                  object \n",
      " 15  hour_of_day             int64  \n",
      " 16  day_of_week             int64  \n",
      " 17  is_weekend              bool   \n",
      " 18  is_night                bool   \n",
      " 19  monthly_income          float64\n",
      " 20  amount_vs_income_ratio  float64\n",
      " 21  category_risk_score     float64\n",
      " 22  channel_risk_score      float64\n",
      " 23  entry_mode_risk_score   float64\n",
      " 24  country_risk_score      float64\n",
      " 25  fraud_score             float64\n",
      " 26  fraud_label             int64  \n",
      "dtypes: bool(3), float64(9), int64(4), object(11)\n",
      "memory usage: 632.3+ MB\n",
      "None\n",
      "       transaction_amount           age        income   hour_of_day  \\\n",
      "count        3.399996e+06  3.399996e+06  3.399996e+06  3.399996e+06   \n",
      "mean         1.917586e+02  4.620203e+01  8.253608e+04  1.150572e+01   \n",
      "std          1.948060e+02  1.417069e+01  3.565371e+04  6.922614e+00   \n",
      "min          1.000000e+00  1.800000e+01  1.500000e+04  0.000000e+00   \n",
      "25%          6.509000e+01  3.600000e+01  5.780924e+04  6.000000e+00   \n",
      "50%          1.343900e+02  4.600000e+01  8.216954e+04  1.200000e+01   \n",
      "75%          2.504900e+02  5.600000e+01  1.054454e+05  1.800000e+01   \n",
      "max          2.745440e+03  7.500000e+01  2.196349e+05  2.300000e+01   \n",
      "\n",
      "        day_of_week  monthly_income  amount_vs_income_ratio  \\\n",
      "count  3.399996e+06    3.399996e+06            3.399996e+06   \n",
      "mean   3.001435e+00    6.878007e+03            2.577572e-02   \n",
      "std    1.996442e+00    2.971142e+03            2.105172e-02   \n",
      "min    0.000000e+00    1.250000e+03            4.106824e-04   \n",
      "25%    1.000000e+00    4.817437e+03            1.179836e-02   \n",
      "50%    3.000000e+00    6.847462e+03            1.979761e-02   \n",
      "75%    5.000000e+00    8.787116e+03            3.276576e-02   \n",
      "max    6.000000e+00    1.830291e+04            1.500009e-01   \n",
      "\n",
      "       category_risk_score  channel_risk_score  entry_mode_risk_score  \\\n",
      "count         3.399996e+06        3.399996e+06           3.399996e+06   \n",
      "mean          8.621419e-01        1.280224e+00           1.514381e+00   \n",
      "std           4.660604e-01        5.930585e-01           1.039031e+00   \n",
      "min           4.000000e-01        7.000000e-01           5.000000e-01   \n",
      "25%           5.000000e-01        7.000000e-01           5.000000e-01   \n",
      "50%           7.000000e-01        1.500000e+00           1.000000e+00   \n",
      "75%           1.500000e+00        2.000000e+00           3.000000e+00   \n",
      "max           2.000000e+00        2.000000e+00           3.000000e+00   \n",
      "\n",
      "       country_risk_score   fraud_score   fraud_label  \n",
      "count        3.399996e+06  3.399996e+06  3.399996e+06  \n",
      "mean         6.596775e-01  4.949144e+00  4.687506e-02  \n",
      "std          1.578312e-01  3.662945e+00  2.113712e-01  \n",
      "min          6.000000e-01  0.000000e+00  0.000000e+00  \n",
      "25%          6.000000e-01  2.000000e+00  0.000000e+00  \n",
      "50%          6.000000e-01  4.000000e+00  0.000000e+00  \n",
      "75%          6.000000e-01  8.400000e+00  0.000000e+00  \n",
      "max          1.400000e+00  1.480000e+01  1.000000e+00  \n",
      "fraud_label\n",
      "0    3240621\n",
      "1     159375\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Dataframe info\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "print(df['fraud_label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b03599",
   "metadata": {},
   "source": [
    "##### Temporal train and test split (based on transaction_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "64d4eab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: (2719997, 27), Test set size: (679999, 27)\n"
     ]
    }
   ],
   "source": [
    "# Temporal train and test split (based on transaction_timestamp)\n",
    "# Still indicating percentage split, but using time-based criteria\n",
    "df['transaction_timestamp'] = pd.to_datetime(df['transaction_timestamp'])\n",
    "split_date = df['transaction_timestamp'].quantile(0.8)\n",
    "train_df = df[df['transaction_timestamp'] <= split_date]\n",
    "test_df = df[df['transaction_timestamp'] > split_date]\n",
    "print(f\"Train set size: {train_df.shape}, Test set size: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe77af8",
   "metadata": {},
   "source": [
    "##### Remove columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "329a1a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['transaction_amount', 'merchant_category', 'channel', 'entry_mode',\n",
      "       'transaction_country', 'is_international', 'age', 'income', 'region',\n",
      "       'hour_of_day', 'day_of_week', 'is_weekend', 'is_night',\n",
      "       'monthly_income', 'amount_vs_income_ratio', 'fraud_label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Remove useless columns, risk features and fraud heuristics (TRAIN SET)\n",
    "train_df = train_df.drop(columns=['customer_id', 'transaction_id', 'transaction_timestamp', 'transaction_status', 'signup_date', 'customer_name', 'category_risk_score', 'channel_risk_score', 'entry_mode_risk_score', 'country_risk_score', 'fraud_score'])\n",
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f67eee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['transaction_amount', 'merchant_category', 'channel', 'entry_mode',\n",
      "       'transaction_country', 'is_international', 'age', 'income', 'region',\n",
      "       'hour_of_day', 'day_of_week', 'is_weekend', 'is_night',\n",
      "       'monthly_income', 'amount_vs_income_ratio', 'fraud_label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Remove useless columns, risk features and fraud heuristics (TEST SET)\n",
    "test_df = test_df.drop(columns=['customer_id', 'transaction_id', 'transaction_timestamp', 'transaction_status', 'signup_date', 'customer_name', 'category_risk_score', 'channel_risk_score', 'entry_mode_risk_score', 'country_risk_score', 'fraud_score'])\n",
    "print(test_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd09a4b",
   "metadata": {},
   "source": [
    "##### Generate X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "706fece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df.drop(columns=['fraud_label'])\n",
    "y_train = train_df['fraud_label']\n",
    "\n",
    "x_test = test_df.drop(columns=['fraud_label'])\n",
    "y_test = test_df['fraud_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ac1383d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (2719997, 15), y_train shape: (2719997,)\n",
      "x_test shape: (679999, 15), y_test shape: (679999,)\n"
     ]
    }
   ],
   "source": [
    "# Visualize train vs testing data\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bb7dab",
   "metadata": {},
   "source": [
    "#### Explore models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c1f6b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "NUMERIC_FEATURES = [\n",
    "    \"transaction_amount\",\n",
    "    \"monthly_income\",\n",
    "    \"amount_vs_income_ratio\",\n",
    "    \"age\",\n",
    "    \"income\",\n",
    "    \"hour_of_day\",\n",
    "    \"day_of_week\",\n",
    "]\n",
    "\n",
    "BOOLEAN_FEATURES = [\n",
    "    \"is_international\",\n",
    "    \"is_weekend\",\n",
    "    \"is_night\",\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    \"merchant_category\",\n",
    "    \"channel\",\n",
    "    \"entry_mode\",\n",
    "    \"transaction_country\",\n",
    "    \"region\",\n",
    "]\n",
    "\n",
    "def build_preprocessor():\n",
    "    numeric_transformer = StandardScaler()\n",
    "    \n",
    "    categorical_transformer = OneHotEncoder(\n",
    "        handle_unknown=\"ignore\",\n",
    "        sparse_output=True\n",
    "    )\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, NUMERIC_FEATURES),\n",
    "            (\"cat\", categorical_transformer, CATEGORICAL_FEATURES),\n",
    "            (\"bool\", \"passthrough\", BOOLEAN_FEATURES),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "    return preprocessor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9e72bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def precision_at_k(y_true, y_scores, k):\n",
    "    k = min(k, len(y_scores))\n",
    "    idx = np.argsort(-y_scores)\n",
    "    top_k_idx = idx[:k]\n",
    "    return y_true[top_k_idx].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba43afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all these models if your PC can handle it (depending on dataset)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "models = {\n",
    "    \"logreg\": (\n",
    "        LogisticRegression(max_iter=1000, class_weight=\"balanced\"),\n",
    "        {\"model__C\": [0.1, 1.0, 10.0]}\n",
    "    ),\n",
    "    \"random_forest\": (\n",
    "        RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            n_jobs=-1,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=42\n",
    "        ),\n",
    "        {\n",
    "            \"model__max_depth\": [5, 10, None],\n",
    "            \"model__min_samples_split\": [2, 10]\n",
    "        }\n",
    "    ),\n",
    "    \"gradient_boosting\": (\n",
    "        GradientBoostingClassifier(random_state=42),\n",
    "        {\n",
    "            \"model__n_estimators\": [100, 200],\n",
    "            \"model__learning_rate\": [0.05, 0.1],\n",
    "            \"model__max_depth\": [2, 3]\n",
    "        }\n",
    "    )\n",
    "}\n",
    "\n",
    "# XGBoost opcional\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    models[\"xgboost\"] = (\n",
    "        XGBClassifier(\n",
    "            n_estimators=300,\n",
    "            eval_metric=\"logloss\",\n",
    "            tree_method=\"hist\",\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        ),\n",
    "        {\n",
    "            \"model__max_depth\": [3, 5],\n",
    "            \"model__learning_rate\": [0.05, 0.1],\n",
    "            \"model__subsample\": [0.8, 1.0],\n",
    "            \"model__colsample_bytree\": [0.8, 1.0],\n",
    "        }\n",
    "    )\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6f2337b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am only trying logreg because of compute power limitations\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "models = {\n",
    "    \"logreg\": (\n",
    "        LogisticRegression(max_iter=1000, class_weight=\"balanced\"),\n",
    "        {\"model__C\": [0.1, 1.0, 10.0]}\n",
    "    )}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c3d6f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "def train_and_evaluate_all(X_train, y_train, X_test, y_test, k_precision=100):\n",
    "    preprocessor = build_preprocessor()\n",
    "\n",
    "    for name, (estimator, params) in models.items():\n",
    "        print(\"=\"*80)\n",
    "        print(f\"TRAINING MODEL → {name}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            (\"preprocess\", preprocessor),\n",
    "            (\"model\", estimator),\n",
    "        ])\n",
    "\n",
    "        grid = GridSearchCV(\n",
    "            estimator=pipe,\n",
    "            param_grid=params,\n",
    "            scoring=\"roc_auc\",\n",
    "            cv=3,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        grid.fit(X_train, y_train)\n",
    "\n",
    "        print(\"\\nBest Params:\", grid.best_params_)\n",
    "        print(\"Best CV AUC:\", grid.best_score_)\n",
    "\n",
    "        best_model = grid.best_estimator_\n",
    "\n",
    "        # ---- Test evaluation ----\n",
    "        y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "        print(f\"\\n[Test] AUC: {auc:.4f}\")\n",
    "        print(f\"[Test] Precision: {prec:.4f}\")\n",
    "        print(f\"[Test] Recall: {rec:.4f}\")\n",
    "\n",
    "        k = min(k_precision, len(y_test))\n",
    "        p_at_k = precision_at_k(y_test.to_numpy(), y_proba, k)\n",
    "        print(f\"[Test] Precision@{k}: {p_at_k:.4f}\")\n",
    "\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd6446c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING MODEL → logreg\n",
      "================================================================================\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "\n",
      "Best Params: {'model__C': 1.0}\n",
      "Best CV AUC: 0.9999902587738866\n",
      "\n",
      "[Test] AUC: 1.0000\n",
      "[Test] Precision: 0.9960\n",
      "[Test] Recall: 1.0000\n",
      "[Test] Precision@100: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[647964    127]\n",
      " [     0  31908]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    648091\n",
      "           1       1.00      1.00      1.00     31908\n",
      "\n",
      "    accuracy                           1.00    679999\n",
      "   macro avg       1.00      1.00      1.00    679999\n",
      "weighted avg       1.00      1.00      1.00    679999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Execute models\n",
    "train_and_evaluate_all(x_train, y_train, x_test, y_test, k_precision=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
